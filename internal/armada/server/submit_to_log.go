package server

import (
	"context"
	"math"

	"github.com/apache/pulsar-client-go/pulsar"
	"github.com/gogo/protobuf/proto"
	"github.com/gogo/protobuf/types"
	"github.com/oklog/ulid"
	"github.com/pkg/errors"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
	v1 "k8s.io/api/core/v1"

	"github.com/G-Research/armada/internal/armada/permissions"
	"github.com/G-Research/armada/internal/armada/repository"
	"github.com/G-Research/armada/internal/common/armadaerrors"
	"github.com/G-Research/armada/internal/common/auth/authorization"
	"github.com/G-Research/armada/internal/common/auth/permission"
	"github.com/G-Research/armada/internal/common/requestid"
	"github.com/G-Research/armada/internal/common/util"
	"github.com/G-Research/armada/internal/events"
	executorconfig "github.com/G-Research/armada/internal/executor/configuration"
	executorutil "github.com/G-Research/armada/internal/executor/util"
	"github.com/G-Research/armada/pkg/api"
	"github.com/G-Research/armada/pkg/client/queue"
)

// Id used for runs for which one was not generated by the system
// (currently used for all runs, since Armada does not generate run ids).
const LEGACY_RUN_ID = "00000000000000000000000000"

// Server that accepts API calls according to the original Armada submit API
// and publishes messages to Pulsar based on those calls.
// TODO: Consider returning a list of message ids of the messages generated
type PulsarSubmitServer struct {
	api.UnimplementedSubmitServer
	Producer        pulsar.Producer
	Permissions     authorization.PermissionChecker
	QueueRepository repository.QueueRepository
	// Fall back to the legacy submit server for queue administration endpoints.
	SubmitServer *SubmitServer
	// Used to create k8s objects from the submitted ingresses and services.
	IngressConfig *executorconfig.IngressConfiguration
}

// TODO: Add input validation to make sure messages can be inserted to the database.
// TODO: Check job size and reject jobs that could never be scheduled. Maybe by querying the scheduler for its limits.
func (srv *PulsarSubmitServer) SubmitJobs(ctx context.Context, req *api.JobSubmitRequest) (*api.JobSubmitResponse, error) {

	userId, groups, err := srv.Authorize(ctx, req.Queue, permissions.SubmitAnyJobs, queue.PermissionVerbSubmit)
	if err != nil {
		return nil, err
	}

	// Prepare an event sequence to be submitted to the log
	sequence := &events.EventSequence{
		Queue:           req.Queue,
		JobSetName:      req.JobSetId,
		QueueJobSetHash: events.HashFromQueueJobSetName(req.Queue, req.JobSetId),
		UserId:          userId,
		Groups:          groups,
		Events:          make([]*events.EventSequence_Event, len(req.JobRequestItems), len(req.JobRequestItems)),
	}

	// Populate the events in the sequence
	responses := make([]*api.JobSubmitResponseItem, len(req.JobRequestItems), len(req.JobRequestItems))
	for i, r := range req.JobRequestItems {
		jobId := util.ULID()
		responses[i] = &api.JobSubmitResponseItem{}
		responses[i].JobId = jobId.String()

		// Create k8s objects from the data embedded in the request.
		// GenerateIngresses expects a job object, but only uses a subset of its fields.
		// Hence, we create a job object with the needed fields populated.
		job := &api.Job{
			Ingress:     r.Ingress,
			Services:    r.Services,
			Labels:      r.Labels,
			Annotations: r.Annotations,
			JobSetId:    req.JobSetId,
			Owner:       userId,
			Namespace:   r.Namespace,
		}
		pod := &v1.Pod{
			Spec: *r.PodSpec,
		}
		pod.Labels = map[string]string{} // TODO: Do we need to put something here?
		services, ingresses := executorutil.GenerateIngresses(job, pod, srv.IngressConfig)

		// Move those objects into a list that can be submitted to the log.
		objects := make([]*events.KubernetesObject, 0, len(services)+len(ingresses))
		for _, service := range services {
			// Default to using the ObjectMeta details provided in the job.
			objectMeta := &events.ObjectMeta{
				Namespace:   r.Namespace,
				Annotations: r.Annotations,
				Labels:      r.Labels,
			}

			// Override the defaults with any info provided from executorutil.GenerateIngresses.
			if service.ObjectMeta.Namespace != "" {
				objectMeta.Namespace = service.ObjectMeta.Namespace
			}
			if service.ObjectMeta.Annotations != nil {
				objectMeta.Annotations = service.ObjectMeta.Annotations
			}
			if service.ObjectMeta.Labels != nil {
				objectMeta.Labels = service.ObjectMeta.Labels
			}

			objects = append(objects, &events.KubernetesObject{
				ObjectMeta: objectMeta,
				Object: &events.KubernetesObject_Service{
					Service: &service.Spec,
				},
			})
		}
		for _, ingress := range ingresses {
			// Default to using the ObjectMeta details provided in the job.
			objectMeta := &events.ObjectMeta{
				Namespace:   r.Namespace,
				Annotations: r.Annotations,
				Labels:      r.Labels,
			}

			// Override the defaults with any info provided from executorutil.GenerateIngresses.
			if ingress.ObjectMeta.Namespace != "" {
				objectMeta.Namespace = ingress.ObjectMeta.Namespace
			}
			if ingress.ObjectMeta.Annotations != nil {
				objectMeta.Annotations = ingress.ObjectMeta.Annotations
			}
			if ingress.ObjectMeta.Labels != nil {
				objectMeta.Labels = ingress.ObjectMeta.Labels
			}

			objects = append(objects, &events.KubernetesObject{
				ObjectMeta: objectMeta,
				Object: &events.KubernetesObject_Ingress{
					Ingress: &ingress.Spec,
				},
			})
		}

		// Each job has a main object associated with it, which determines when the job exits.
		// If provided, use r.PodSpec as the main object. Otherwise, try to use r.PodSpecs[0].
		mainPodSpec := r.PodSpec
		additionalPodSpecs := r.PodSpecs
		if additionalPodSpecs == nil {
			additionalPodSpecs = make([]*v1.PodSpec, 0)
		}
		if mainPodSpec == nil && len(additionalPodSpecs) > 0 {
			mainPodSpec = additionalPodSpecs[0]
			additionalPodSpecs = additionalPodSpecs[1:]
		}
		mainObject := &events.KubernetesMainObject{
			Object: &events.KubernetesMainObject_PodSpec{
				PodSpec: &events.PodSpecWithAvoidList{
					PodSpec: mainPodSpec,
				},
			},
		}

		// Add any additional pod specs into the list of additional objects.
		for _, podSpec := range additionalPodSpecs {
			objects = append(objects, &events.KubernetesObject{
				Object: &events.KubernetesObject_PodSpec{
					PodSpec: &events.PodSpecWithAvoidList{
						PodSpec: podSpec,
					},
				},
			})
		}

		priority, err := parsePriority(r.Priority)
		if err != nil {
			return nil, err
		}

		// Fully formed job creation event to be added to the sequence.
		submitJob := &events.SubmitJob{
			JobId:           events.ProtoUuidFromUlid(jobId),
			DeduplicationId: r.ClientId,
			Priority:        priority,
			ObjectMeta: &events.ObjectMeta{
				Namespace:   r.Namespace,
				Annotations: r.Labels,
				Labels:      r.Annotations,
			},
			MainObject: mainObject,
			Objects:    objects,
		}
		sequence.Events[i] = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_SubmitJob{
				SubmitJob: submitJob,
			},
		}
	}

	payload, err := proto.Marshal(sequence)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to marshal event sequence")
	}

	// Incoming gRPC requests are annotated with a unique id.
	// Pass this id through the log by adding it to the Pulsar message properties.
	requestId, ok := requestid.FromContext(ctx)
	if !ok {
		requestId = "missing"
	}
	_, err = srv.Producer.Send(ctx, &pulsar.ProducerMessage{
		Payload:    payload,
		Properties: map[string]string{requestid.MetadataKey: requestId},
	})
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to send message")
	}

	return &api.JobSubmitResponse{JobResponseItems: responses}, nil
}

func (srv *PulsarSubmitServer) CancelJobs(ctx context.Context, req *api.JobCancelRequest) (*api.CancellationResult, error) {
	userId, groups, err := srv.Authorize(ctx, req.Queue, permissions.CancelAnyJobs, queue.PermissionVerbCancel)
	if err != nil {
		return nil, err
	}

	sequence := &events.EventSequence{
		Queue:           req.Queue,
		JobSetName:      req.JobSetId,
		QueueJobSetHash: events.HashFromQueueJobSetName(req.Queue, req.JobSetId),
		UserId:          userId,
		Groups:          groups,
		Events:          make([]*events.EventSequence_Event, 1, 1),
	}

	jobId, _, err := parseJobRunIds(req.JobId)
	if err != nil {
		return nil, err
	}

	sequence.Events[1] = &events.EventSequence_Event{
		Event: &events.EventSequence_Event_CancelJob{
			CancelJob: &events.CancelJob{JobId: events.ProtoUuidFromUlid(jobId)},
		},
	}

	payload, err := proto.Marshal(sequence)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to marshal event sequence")
	}

	// Incoming gRPC requests are annotated with a unique id.
	// Pass this id through the log by adding it to the Pulsar message properties.
	requestId, ok := requestid.FromContext(ctx)
	if !ok {
		requestId = "missing"
	}
	_, err = srv.Producer.Send(ctx, &pulsar.ProducerMessage{
		Payload:    payload,
		Properties: map[string]string{requestid.MetadataKey: requestId},
	})
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to send message")
	}

	return &api.CancellationResult{
		CancelledIds: []string{req.JobId},
	}, nil
}

func (srv *PulsarSubmitServer) ReprioritizeJobs(ctx context.Context, req *api.JobReprioritizeRequest) (*api.JobReprioritizeResponse, error) {
	userId, groups, err := srv.Authorize(ctx, req.Queue, permissions.ReprioritizeAnyJobs, queue.PermissionVerbReprioritize)
	if err != nil {
		return nil, err
	}

	// TODO: What gets put in the results?
	results := make(map[string]string)
	sequence := &events.EventSequence{
		Queue:           req.Queue,
		JobSetName:      req.JobSetId,
		QueueJobSetHash: events.HashFromQueueJobSetName(req.Queue, req.JobSetId),
		UserId:          userId,
		Groups:          groups,
		Events:          make([]*events.EventSequence_Event, len(req.JobIds), len(req.JobIds)),
	}

	priority, err := parsePriority(req.NewPriority)
	if err != nil {
		return nil, err
	}

	for i, jobIdString := range req.JobIds {

		jobId, _, err := parseJobRunIds(jobIdString)
		if err != nil {
			results[jobIdString] = err.Error()
			continue
		}

		sequence.Events[i] = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_ReprioritiseJob{
				ReprioritiseJob: &events.ReprioritiseJob{
					JobId:    events.ProtoUuidFromUlid(jobId),
					Priority: priority,
				},
			},
		}

		results[jobIdString] = "" // TODO: what do we put here?
	}

	payload, err := proto.Marshal(sequence)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to marshal event sequence")
	}

	// Incoming gRPC requests are annotated with a unique id.
	// Pass this id through the log by adding it to the Pulsar message properties.
	requestId, ok := requestid.FromContext(ctx)
	if !ok {
		requestId = "missing"
	}
	_, err = srv.Producer.Send(ctx, &pulsar.ProducerMessage{
		Payload:    payload,
		Properties: map[string]string{requestid.MetadataKey: requestId},
	})
	if err != nil {
		return nil, status.Errorf(codes.Internal, "failed to send message")
	}

	return &api.JobReprioritizeResponse{
		ReprioritizationResults: results,
	}, nil
}

// Authorize authorizes a user request to submit a state transition message to the log.
// User information used for authorization is extracted from the provided context.
// Checks that the user has either anyPerm (e.g., permissions.SubmitAnyJobs) or perm (e.g., PermissionVerbSubmit) for this queue.
// Returns the userId and groups extracted from the context.
func (srv *PulsarSubmitServer) Authorize(ctx context.Context, queueName string, anyPerm permission.Permission, perm queue.PermissionVerb) (userId string, groups []string, err error) {
	principal := authorization.GetPrincipal(ctx)
	userId = principal.GetName()
	q, err := srv.QueueRepository.GetQueue(queueName)
	if err != nil {
		return
	}
	if !srv.Permissions.UserHasPermission(ctx, anyPerm) {
		if !principalHasQueuePermissions(principal, q, perm) {
			// TODO: Add an error to armadaerrors and use that instead. Use sibling errors instead of a list of reasons.
			err = &armadaerrors.ErrNoPermission{
				Principal:  principal.GetName(),
				Permission: string(perm),
				Action:     string(perm) + " for queue " + q.Name,
				Message:    "",
			}
			err = errors.WithStack(err)
			return
		}
	}

	// Armada impersonates the principal that submitted the job when interacting with k8s.
	// If the principal doesn't itself have sufficient perms, we check if it's part of any groups that do, and add those.
	// This is an optimisation to avoid passing around groups unnecessarily.
	principalSubject := queue.PermissionSubject{
		Name: userId,
		Kind: queue.PermissionSubjectKindUser,
	}
	if !q.HasPermission(principalSubject, perm) {
		for _, subject := range queue.NewPermissionSubjectsFromOwners(nil, principal.GetGroupNames()) {
			if q.HasPermission(subject, perm) {
				groups = append(groups, subject.Name)
			}
		}
	}
	return
}

// principalHasQueuePermissions returns true if the principal has permissions to perform some action,
// as specified by the provided verb, for a specific queue, and false otherwise.
func principalHasQueuePermissions(principal authorization.Principal, q queue.Queue, verb queue.PermissionVerb) bool {
	subjects := queue.PermissionSubjects{}
	for _, group := range principal.GetGroupNames() {
		subjects = append(subjects, queue.PermissionSubject{
			Name: group,
			Kind: queue.PermissionSubjectKindGroup,
		})
	}
	subjects = append(subjects, queue.PermissionSubject{
		Name: principal.GetName(),
		Kind: queue.PermissionSubjectKindUser,
	})

	for _, subject := range subjects {
		if q.HasPermission(subject, verb) {
			return true
		}
	}

	return false
}

// Fallback methods. Calls into an embedded server.SubmitServer.
func (srv *PulsarSubmitServer) CreateQueue(ctx context.Context, req *api.Queue) (*types.Empty, error) {
	return srv.SubmitServer.CreateQueue(ctx, req)
}
func (srv *PulsarSubmitServer) UpdateQueue(ctx context.Context, req *api.Queue) (*types.Empty, error) {
	return srv.SubmitServer.UpdateQueue(ctx, req)
}
func (srv *PulsarSubmitServer) DeleteQueue(ctx context.Context, req *api.QueueDeleteRequest) (*types.Empty, error) {
	return srv.SubmitServer.DeleteQueue(ctx, req)
}
func (srv *PulsarSubmitServer) GetQueue(ctx context.Context, req *api.QueueGetRequest) (*api.Queue, error) {
	return srv.SubmitServer.GetQueue(ctx, req)
}
func (srv *PulsarSubmitServer) GetQueueInfo(ctx context.Context, req *api.QueueInfoRequest) (*api.QueueInfo, error) {
	return srv.GetQueueInfo(ctx, req)
}

// SubmitApiEvent converts an api.EventMessage into Pulsar state transition messages and publishes those to Pulsar.
func (srv *PulsarSubmitServer) SubmitApiEvent(ctx context.Context, apiEvent *api.EventMessage) error {
	sequence, err := PulsarSequenceFromApiEvent(apiEvent)
	if err != nil {
		return err
	}

	payload, err := proto.Marshal(sequence)
	if err != nil {
		err = errors.WithStack(err)
		return err
	}

	// Incoming gRPC requests are annotated with a unique id.
	// Pass this id through the log by adding it to the Pulsar message properties.
	requestId, ok := requestid.FromContext(ctx)
	if !ok {
		requestId = "missing"
	}
	_, err = srv.Producer.Send(ctx, &pulsar.ProducerMessage{
		Payload:    payload,
		Properties: map[string]string{requestid.MetadataKey: requestId},
	})
	if err != nil {
		err = errors.WithStack(err)
		return err
	}

	return nil
}

// PulsarSequenceFromApiEvent converts an api.EventMessage into the corresponding Pulsar event
// and returns an EventSequence containing this single event.
//
// TODO: The following two lists are not updated.
// The following is a list of API messages. Those marked with x are handled by this function.
//	*EventMessage_Submitted
//	*EventMessage_Queued
//	*EventMessage_DuplicateFound
// x	*EventMessage_Leased
// x	*EventMessage_LeaseReturned
// x	*EventMessage_LeaseExpired
// x	*EventMessage_Pending
// x	*EventMessage_Running
// x	*EventMessage_UnableToSchedule
// x	*EventMessage_Failed
// x	*EventMessage_Succeeded
//	*EventMessage_Reprioritized
//	*EventMessage_Cancelling
//	*EventMessage_Cancelled
// x	*EventMessage_Terminated
//	*EventMessage_Utilisation
//	*EventMessage_IngressInfo
//	*EventMessage_Reprioritizing
//	*EventMessage_Updated
//
// The following is a list of Pulsar events. Those marked with x may be returned by this function.
//	*EventSequence_Event_JobSucceeded
// x	*EventSequence_Event_JobFailed
//	*EventSequence_Event_JobRejected
// x	*EventSequence_Event_JobRunLeased
// x	*EventSequence_Event_JobRunAssigned
// x	*EventSequence_Event_JobRunRunning
// x	*EventSequence_Event_JobRunReturned
// x	*EventSequence_Event_JobRunSucceeded
// x	*EventSequence_Event_JobRunFailed
func PulsarSequenceFromApiEvent(msg *api.EventMessage) (sequence *events.EventSequence, err error) {
	sequence = &events.EventSequence{}
	var event *events.EventSequence_Event

	switch m := msg.Events.(type) {
	case *api.EventMessage_Leased:
		sequence.Queue = m.Leased.Queue
		sequence.JobSetName = m.Leased.JobSetId

		jobId, runId, err := parseJobRunIds(m.Leased.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunLeased{
				JobRunLeased: &events.JobRunLeased{
					RunId:      events.ProtoUuidFromUlid(runId),
					JobId:      events.ProtoUuidFromUlid(jobId),
					ExecutorId: m.Leased.ClusterId,
				},
			},
		}
	case *api.EventMessage_LeaseReturned:
		sequence.Queue = m.LeaseReturned.Queue
		sequence.JobSetName = m.LeaseReturned.JobSetId

		jobId, runId, err := parseJobRunIds(m.LeaseReturned.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunErrors{
				JobRunErrors: &events.JobRunErrors{
					RunId:    events.ProtoUuidFromUlid(runId),
					JobId:    events.ProtoUuidFromUlid(jobId),
					Terminal: true, // EventMessage_LeaseReturned indicates a failed job run.
					Errors: []*events.Error{
						{
							Code:    0,
							Message: m.LeaseReturned.Reason,
							ObjectMeta: &events.ObjectMeta{
								Namespace:    "",
								Name:         "",
								KubernetesId: m.LeaseReturned.KubernetesId,
								Annotations:  nil,
								Labels:       nil,
							},
						},
					},
				},
			},
		}
	case *api.EventMessage_LeaseExpired:
		sequence.Queue = m.LeaseExpired.Queue
		sequence.JobSetName = m.LeaseExpired.JobSetId

		jobId, runId, err := parseJobRunIds(m.LeaseExpired.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunErrors{
				JobRunErrors: &events.JobRunErrors{
					RunId:    events.ProtoUuidFromUlid(runId),
					JobId:    events.ProtoUuidFromUlid(jobId),
					Terminal: true, // EventMessage_LeaseExpired indicates a failed job run.
					Errors: []*events.Error{
						{
							Code: 0,
							Reason: &events.Error_LeaseExpired{
								LeaseExpired: &events.LeaseExpired{},
							},
						},
					},
				},
			},
		}
	case *api.EventMessage_Pending:
		sequence.Queue = m.Pending.Queue
		sequence.JobSetName = m.Pending.JobSetId

		jobId, runId, err := parseJobRunIds(m.Pending.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunAssigned{
				JobRunAssigned: &events.JobRunAssigned{
					RunId: events.ProtoUuidFromUlid(runId),
					JobId: events.ProtoUuidFromUlid(jobId),
				},
			},
		}
	case *api.EventMessage_Running:
		sequence.Queue = m.Running.Queue
		sequence.JobSetName = m.Running.JobSetId

		jobId, runId, err := parseJobRunIds(m.Running.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunRunning{
				JobRunRunning: &events.JobRunRunning{
					RunId: events.ProtoUuidFromUlid(runId),
					JobId: events.ProtoUuidFromUlid(jobId),
					ResourceInfos: []*events.KubernetesResourceInfo{
						{
							ObjectMeta: &events.ObjectMeta{
								Namespace:    m.Running.NodeName,
								Name:         m.Running.PodName,
								KubernetesId: m.Running.KubernetesId,
								// TODO: These should be included.
								Annotations: nil,
								Labels:      nil,
							},
							Info: &events.KubernetesResourceInfo_PodInfo{
								PodInfo: &events.PodInfo{
									NodeName:  m.Running.NodeName,
									PodNumber: m.Running.PodNumber,
								},
							},
						},
					},
				},
			},
		}
	case *api.EventMessage_UnableToSchedule:
		sequence.Queue = m.UnableToSchedule.Queue
		sequence.JobSetName = m.UnableToSchedule.JobSetId

		jobId, _, err := parseJobRunIds(m.UnableToSchedule.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobErrors{
				JobErrors: &events.JobErrors{
					JobId:    events.ProtoUuidFromUlid(jobId),
					Terminal: true, // EventMessage_UnableToSchedule indicates a failed job.
					Errors: []*events.Error{
						{
							Code: 0,
							Reason: &events.Error_JobUnschedulable{
								JobUnschedulable: &events.JobUnschedulable{},
							},
						},
					},
				},
			},
		}
	case *api.EventMessage_Failed:
		sequence.Queue = m.Failed.Queue
		sequence.JobSetName = m.Failed.JobSetId

		// EventMessage_Failed may contain several errors (one for each container).
		// Here, we create a tree from those errors which is embedded in a JobErrors message.
		numErrors := len(m.Failed.ContainerStatuses) + 1
		topError := &events.Error{
			Code:        0,
			Message:     m.Failed.Reason,
			Reason:      nil,
			ChildErrors: make([]*events.Error, 0, numErrors),
		}
		for _, st := range m.Failed.ContainerStatuses {
			if st.ExitCode == 0 {
				// This container exited successfully
				continue
			}
			topError.ChildErrors = append(topError.ChildErrors, &events.Error{
				Code:    0,
				Message: st.Message + ", " + st.Reason,
				Reason: &events.Error_ApplicationError{
					ApplicationError: &events.ApplicationError{
						Code:    st.ExitCode,
						Message: st.Reason,
					},
				},
			})
		}

		jobId, runId, err := parseJobRunIds(m.Failed.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunErrors{
				JobRunErrors: &events.JobRunErrors{
					RunId:    events.ProtoUuidFromUlid(runId),
					JobId:    events.ProtoUuidFromUlid(jobId),
					Terminal: true, // EventMessage_Failed indicates a failed job.
					Errors: []*events.Error{
						topError,
					},
				},
			},
		}
	case *api.EventMessage_Succeeded:
		sequence.Queue = m.Succeeded.Queue
		sequence.JobSetName = m.Succeeded.JobSetId

		jobId, runId, err := parseJobRunIds(m.Succeeded.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobRunSucceeded{
				JobRunSucceeded: &events.JobRunSucceeded{
					RunId: events.ProtoUuidFromUlid(runId),
					JobId: events.ProtoUuidFromUlid(jobId),
				},
			},
		}
	case *api.EventMessage_Terminated:
		sequence.Queue = m.Terminated.Queue
		sequence.JobSetName = m.Terminated.JobSetId

		jobId, _, err := parseJobRunIds(m.Terminated.JobId)
		if err != nil {
			return nil, err
		}

		event = &events.EventSequence_Event{
			Event: &events.EventSequence_Event_JobErrors{
				JobErrors: &events.JobErrors{
					JobId:    events.ProtoUuidFromUlid(jobId),
					Terminal: true,
					Errors: []*events.Error{
						{
							Reason: &events.Error_MaxRunsExceeded{
								MaxRunsExceeded: &events.MaxRunsExceeded{},
							},
						},
					},
				},
			},
		}
	default:
		err = &armadaerrors.ErrInvalidArgument{
			Name:    "msg",
			Value:   msg,
			Message: "received unsupported api message",
		}
		err = errors.WithStack(err)
		return nil, err
	}

	sequence.Events = []*events.EventSequence_Event{event}
	return sequence, nil
}

func parseJobId(jobId string) (ulid.ULID, error) {
	id, err := ulid.Parse(jobId)
	if err != nil {
		err = errors.WithStack(err)
	}
	return id, err
}

// parseJobRunIds is a convenience function for converting a string to a ULID and
// then returning the default legacy id for runs without an included id.
// Returns an ErrInvalidArgument if parsing either id fails.
func parseJobRunIds(jobIdString string) (ulid.ULID, ulid.ULID, error) {
	jobId, err := ulid.Parse(jobIdString)
	if err != nil {
		err = &armadaerrors.ErrInvalidArgument{
			Name:    "jobId",
			Value:   jobIdString,
			Message: err.Error(),
		}
		err = errors.WithStack(err)
		return jobId, jobId, err
	}

	runId, err := ulid.Parse(LEGACY_RUN_ID)
	if err != nil {
		err = &armadaerrors.ErrInvalidArgument{
			Name:    "runId",
			Value:   LEGACY_RUN_ID,
			Message: err.Error(),
		}
		err = errors.WithStack(err)
		return jobId, runId, err
	}

	return jobId, runId, nil
}

// parsePriority returns the uint32 representation of the priority included with a submitted job,
// or an error if the conversion fails.
func parsePriority(priority float64) (uint32, error) {
	if priority < 1 {
		return 0, &armadaerrors.ErrInvalidArgument{
			Name:    "priority",
			Value:   priority,
			Message: "priority must be larger than or equal to 1",
		}
	}
	if priority > math.MaxUint32 {
		priority = math.MaxUint32
	}
	priority = math.Round(priority)
	return uint32(priority), nil
}
